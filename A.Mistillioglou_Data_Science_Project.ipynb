{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "185cd36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import matplotlib as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95e248a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the data into pandas dataframes\n",
    "\n",
    "df1 = pd.read_excel(r\"C:\\Users\\Alex\\Desktop\\Data science\\Deliverable\\Data\\R&D ranking of the world top 2500 companies_0_0.xlsx\")\n",
    "df2 = pd.read_excel(r\"C:\\Users\\Alex\\Desktop\\Data science\\Deliverable\\Data\\SB2021_World2500_0.xlsx\")\n",
    "df3 = pd.read_excel(r\"C:\\Users\\Alex\\Desktop\\Data science\\Deliverable\\Data\\SB2020_World2500.xlsx\")\n",
    "df4 = pd.read_excel(r\"C:\\Users\\Alex\\Desktop\\Data science\\Deliverable\\Data\\SB2019_main stats_GLOBAL2500.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedf537e",
   "metadata": {},
   "source": [
    "    Innovation has become ever more critical in the last decades in the efforts of companies to get a competitive advantage over their competitors. Despite that, it would be a massive understatement if we consider innovation as purely a drive for profits, as there are substantial side-effects that it generates. They may be anything from an expected boost in GDP and in the general prosperity of a country to a matter of national security. Therefore, as is expected, governments around the world have used any tools available in order to foster innovation. These tools could vary from anything between direct investments and subsidies or more indirect approaches like tax benefits or policies that are meant to incentivise companies to invest more in research.\n",
    "    One of the most commonly accepted metrics of innovation is the levels of R&D expenditure, as they are easily accessible and quantifiable. For this purpose, in this project, we will attempt to predict the levels of R&D expenditures based on a variety of metrics that are available for some of the biggest companies around the world. In case we find a strong correlation with certain metrics, we will then replicate the process and predict future R&D expenditures with machine learning techniques.\n",
    "    An attempt to predict the future R&D expenditure could be indeed a challenging endeavour as it would require relying on publicly available data that companies have provided. In our case, one of the most reliable sources of data that we could find was the “EU Industrial R&D Investment Scoreboard”, which the European Commission prepares on an annual basis in order to “benchmark the performance of EU innovation-driven industries against major global counterparts and to provide an R&D investment database that companies, investors and policymakers can use to compare individual company performances against the best global competitors in their sectors.”\n",
    "    This dashboard includes data regarding the 2500 largest companies around the world. As these companies’ R&D expenditure for 2020 accounted for 90% of the world’s business-funded R&D, a successful way to predict their future R&D strategies would be an extremely valuable l tool in the hands of policymakers in their efforts to encourage innovation further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2444d91",
   "metadata": {},
   "source": [
    "    Our dataset contains information from the annual reports for the years 2018-2021. The datasets that were used can be easily retrieved from the website\"https://iri.jrc.ec.europa.eu/scoreboard/2021-eu-industrial-rd-investment-scoreboard\". The variables of the datasets include information about the current financial metrics of the company, such as the market capitalisation and the number of employees, the annual growth rate of these metrics and finally, some broader about the broader characteristics of the company like the location that is based in or the industry that it belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dc57ea",
   "metadata": {},
   "source": [
    "      Prior use the data for the regression and the attempt to use the different machine learning techniques, we need to do the appropriate data cleaning. Moreover, in an attempt to make the code as clear and easy to read as possible, we are thorough in our comments below.\n",
    "      We merge the datasets on the columns that they have in common. The reason for this is that we want to approach the problem cross-sectionally and delve deeper into the relationship between the \"R&D one-year growth\" and the other available attributes. Since, in order to examine this relationship, we need a large enough dataset, we decided to use the datasets of the last four years.\n",
    "      It is of paramount importance to point out that , as the report mentions, \"In 2020, the pandemic hit global business hard, causing a significant drop in companies' sales, profits and capital expenditures. However, overall R&D investment was sustained by increases in sectors positively affected by the crisis, namely ICT services and Health industries, while most other sectors decreased R&D investment, particularly the transport-related industries that have been most strongly affected by the lockdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43873fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since our dataframes have different columns we are going to do a crude concat to append the datasets on the columns\n",
    "\n",
    "data_whole = pd.concat([df1, df2,df3,df4],ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2228d3c",
   "metadata": {},
   "source": [
    "    Regading the assumptions that we have to make, the most vital one, would be that it is assumed that the observations with the missing values are not correlated, as if they are, their removal would lead to bias. Additionally, a rather safer assumption that we have to make is about the reliability of our data source, but as the data source is from the European Commission, it is assumed that it is as reliable as it can be. The positive aspect would be that we would not have to make any assumptions regarding the causal relationship between our variables, as the aim of this project is to simply use machine learning to predict the values and not to infer a causal relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7d60351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['R&D one-year growth (%)', 'Net sales one-year growth (%)',\n",
       "       'R&D intensity (%)', 'Capex one-year growth (%)', 'Capex intensity (%)',\n",
       "       'Op.profits one-year growth (%)', 'Profitability (%)', 'Employees',\n",
       "       'Employees one-year growth (%)', 'Market cap one-year growth (%)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dropping the attributes with less that 8000 non nun values\n",
    "\n",
    "data_whole.dropna( inplace = True, axis=1, thresh=8000)\n",
    "\n",
    "#Dropping the rows that contain Nun values\n",
    "\n",
    "data_whole.dropna(inplace = True)\n",
    "data_whole.isna().sum() # Check that we have no nun values left\n",
    "\n",
    "#We can do that because we are still elft with a significant number of rows to conduct predictions\n",
    "#Otherwise we would have to replicate some values and fill them in place of the nun values\n",
    "#We still have 8383 rows\n",
    "\n",
    "#Drop unnecessary columns\n",
    "\n",
    "data_whole = data_whole.drop(['World rank', 'Company', 'Country', 'Region'],axis= 1)\n",
    "data_whole.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "111c9497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the data from panda objects to float64\n",
    "\n",
    "for i in data_whole.columns:\n",
    "    \n",
    "    data_whole[i]= pd.to_numeric(data_whole[i],errors='coerce')\n",
    "    \n",
    "#Droping Na values that could have been created due to the argument errors='coerce'\n",
    "\n",
    "data_whole.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd20c563",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scalling the data\n",
    "\n",
    "data_to_be_scaled = data_whole.to_numpy()\n",
    "\n",
    "scaler = MinMaxScaler(feature_range= (0,1)) #Initiating the scaler\n",
    "data_scaled = scaler.fit_transform(data_to_be_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaf18a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7774, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the shape of our data so we can perform the split\n",
    "data_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b0f3d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data to x and y\n",
    "\n",
    "y = data_scaled[:,0]\n",
    "X = data_scaled[:,1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0cfdeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.010\n",
      "Model:                            OLS   Adj. R-squared:                  0.009\n",
      "Method:                 Least Squares   F-statistic:                     8.768\n",
      "Date:                Fri, 17 Jun 2022   Prob (F-statistic):           3.14e-13\n",
      "Time:                        13:39:08   Log-Likelihood:                 20432.\n",
      "No. Observations:                7774   AIC:                        -4.084e+04\n",
      "Df Residuals:                    7764   BIC:                        -4.077e+04\n",
      "Df Model:                           9                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0121      0.044      0.272      0.786      -0.075       0.099\n",
      "x1             0.0079      0.011      0.734      0.463      -0.013       0.029\n",
      "x2             0.0009      0.049      0.018      0.986      -0.095       0.097\n",
      "x3             0.0730      0.011      6.472      0.000       0.051       0.095\n",
      "x4             0.0048      0.018      0.265      0.791      -0.031       0.040\n",
      "x5             0.0103      0.015      0.694      0.487      -0.019       0.039\n",
      "x6            -0.0012      0.045     -0.027      0.978      -0.089       0.086\n",
      "x7            -0.0025      0.003     -0.964      0.335      -0.008       0.003\n",
      "x8             0.0066      0.017      0.381      0.704      -0.028       0.041\n",
      "x9             0.0454      0.010      4.659      0.000       0.026       0.065\n",
      "==============================================================================\n",
      "Omnibus:                    21122.201   Durbin-Watson:                   1.997\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):        784000094.358\n",
      "Skew:                          33.429   Prob(JB):                         0.00\n",
      "Kurtosis:                    1557.318   Cond. No.                         572.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "#A regression\n",
    "\n",
    "\n",
    "X2 = sm.add_constant(X) # adding a constant\n",
    "\n",
    "est = sm.OLS(y, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary()) # This allows us to actually see the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4dcfa2",
   "metadata": {},
   "source": [
    "    In this part we are splitting the data into training and testing using the classic train_test_split of sklearn and scale the data appropriately to fit into the machine learning methods we are going to attempt, as not doing that will significally impact the result of such methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f27b0b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine learning attempt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#spliting to train and test data , test size 20%\n",
    "\n",
    "ML_x = y = data_to_be_scaled[:,1:10]\n",
    "\n",
    "ML_y = data_to_be_scaled[:,0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(ML_x, ML_y, test_size=0.2, random_state=50)\n",
    "\n",
    "#Scaling our data\n",
    "\n",
    "scaler2 = MinMaxScaler(feature_range= (0,1))\n",
    "\n",
    "scaler2.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler2.transform(X_train)\n",
    "X_test_scaled = scaler2.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c23b24",
   "metadata": {},
   "source": [
    "    We will use Random Forest Regression as it combines predictions from multiple machine learning algorithms to make a more accurate prediction than a single model. Despite that the presence of correlated predictors has been shown to impact its ability to identify strong predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9f23e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.7749318140132826"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Instantiate model with 500 decision trees\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators = 500, criterion='squared_error', random_state = 42) #Initiating the mode\n",
    "# Train the model on training data\n",
    "\n",
    "rf.fit(X_train_scaled, y_train) #Training the model\n",
    "rf_predictions = rf.predict(X_test_scaled) #Conducting predictions on test data\n",
    "\n",
    "from sklearn.metrics import r2_score \n",
    "\n",
    "r2_score(y_test,rf_predictions) #Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a05472e",
   "metadata": {},
   "source": [
    "    Since in the simple linear we get a better R2 than in the random forest I decide to try two other linear regression Machine learning methods provided by sklearn the Lasso and Ridge models to further explore this topic. This model solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819ee95c",
   "metadata": {},
   "source": [
    "    We are using Ridge Regression, as it is a method of estimating the coefficients in cases that independent variables are highly correlated, as could be the case with our data that reflects the financial health of a company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "157189ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0427909250889289"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ridge Regression ML\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_reg = Ridge(alpha=1) #Initiating the model\n",
    "\n",
    "ridge_reg.fit(X_train_scaled, y_train) #Training the model\n",
    "\n",
    "ridge_reg_predictions = ridge_reg.predict(X_test_scaled) #Conducting predictions on test data\n",
    "r2_score(y_test,ridge_reg_predictions) #Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fafa731",
   "metadata": {},
   "source": [
    "    We are also using Lasso regression to further ensure the robustness of our final results and observe potential differences between the performance of such methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74a3f08f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.034833694128738935"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lasso regression ML\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "\n",
    "lasso_reg.fit(X_train_scaled, y_train) #Training the model\n",
    "\n",
    "lasso_reg_predictions = lasso_reg.predict(X_test_scaled) #Conducting predictions on test data\n",
    "r2_score(y_test,lasso_reg_predictions) #Evaluating the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81a450d",
   "metadata": {},
   "source": [
    "    In conclusion, as machine learning has a larger R-squared, it indicates that the methods better explain the data and are able to conduct predictions. We believe that machine learning can indeed be used for the purpose of predicting the future R&D expenditure of a company and this tool can be used by the policymakers to guide them in creating policies that are aimed in fostering innovation.\n",
    "    For the alpha of both Ridge and Lasso we observe approximately similar values of R-squared with alpha between 0.1 and 1  and therefore, we utilize the one with the better results. We are using only the R-squared, as it provides more precise results, compared to metrics such as the RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3d005d",
   "metadata": {},
   "source": [
    "    Of course, it is essential to emphasise the shortcoming of our analysis. The main issues of this analysis can be attributed to the data, as there was a considerable constraint regarding the availability of the data, while for the existing datasets, there were a number of missing values that could further hamper the reliability of our result. Moreover, despite the same source (EU Commission), the attributes varied yearly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965f26b2",
   "metadata": {},
   "source": [
    "    Resulting from that, we can see that this project would have great potential for more thorough research in the future with better data accessibility and quality, while better computer processing power would ensure that the machine learning would be able to be performed in a more detailed manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0894c419",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
